{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a linear dimensionality reduction technique that can be utilized for extracting information from a high-dimensional space by projecting it into a lower-dimensional sub-space. It tries to preserve the essential parts that have more variation of the data and remove the non-essential parts with fewer variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions are nothing but features that represent the data. For example, A 28 X 28 image has 784 picture elements (pixels) that are the dimensions or features which together represent that image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to note about PCA is that it is an Unsupervised dimensionality reduction technique, you can cluster the similar data points based on the feature correlation between them without any supervision (or labels), and you will learn how to achieve this practically using Python in later sections of this tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to note about PCA is that it is an Unsupervised dimensionality reduction technique, you can cluster the similar data points based on the feature correlation between them without any supervision (or labels).\n",
    "PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. Features, Dimensions, and Variables are all referring to the same thing in this notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main usage of PCA\n",
    "* Data Visualization\n",
    "When working on any data related problem, extensive data exploration like finding out how the variables are correlated or understanding the distribution of a few variables is crucial. Considering that there are a large number of variables or dimensions along which the data is distributed, visualization can be a challenge and almost impossible. Using dimensionality reduction, data can be projected into a lower dimension, thereby allowing you to visualize the data in a 2D or 3D space.\n",
    "\n",
    "\n",
    "* Speeding Machine Learning Algorithm\n",
    "Since PCA's main idea is dimensionality reduction, you can leverage that to speed up your machine learning algorithm's training and testing time considering your data has a lot of features, and the ML algorithm's learning is too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component\n",
    "Principal components are the key to PCA; they represent what's underneath the hood of your data. In a layman term, when the data is projected into a lower dimension (assume three dimensions) from a higher space, the three dimensions are nothing but the three Principal Components that captures (or holds) most of the variance (information) of your data.\n",
    "\n",
    "Principal components have both direction and magnitude. The direction represents across which principal axes the data is mostly spread out or has most variance and the magnitude signifies the amount of variance that Principal Component captures of the data when projected onto that axis. The principal components are a straight line, and the first principal component holds the most variance in the data. Each subsequent principal component is orthogonal to the last and has a lesser variance. In this way, given a set of x correlated variables over y samples you achieve a set of u uncorrelated principal components over the same y samples.\n",
    "\n",
    "The reason you achieve uncorrelated principal components from the original features is that the correlated features contribute to the same principal component, thereby reducing the original data features into uncorrelated principal components; each representing a different set of correlated features with different amounts of variation.\n",
    "\n",
    "Each principal component represents a percentage of total variation captured from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA on iris dataset\n",
    "In this section we will decompose with PCA very simple 4-dimensional data set. This is ono eg the best known pattern recognition dataset. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "#from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset into Pandas DataFrame\n",
    "df_iris = pd.read_csv(iris_url, names=['sepal length', 'sepal width', 'petal length', 'petal width', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_iris.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case that the dimensionality of the data allows it, it is good practice to see how each pair of features correlate with each other. In the followinglink you will find more methods for visualizing multidimensional data using matplotlib and seaborn libraries\n",
    "https://towardsdatascience.com/the-art-of-effective-visualization-of-multi-dimensional-data-6c7202990c57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_iris, hue='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can immediately see that the features petal length and petal width are strongly correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Standardize the Data\n",
    "\n",
    "Since PCA yields a feature subspace that maximizes the variance along the axes, it makes sense to standardize the data, especially, if it was measured on different scales. Although, all features in the Iris dataset were measured in centimeters, let us continue with the transformation of the data onto unit scale (mean=0 and variance=1), which is a requirement for the optimal performance of many machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_iris = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "x_iris = df_iris.loc[:, features_iris].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_iris = df_iris.loc[:, ['target']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_iris = StandardScaler().fit_transform(x_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris_standarize = pd.DataFrame(data=x_iris, columns=features_iris)\n",
    "df_iris_standarize['target'] = df_iris['target']\n",
    "df_iris_standarize.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_iris_standarize, hue='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distributions are now standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Projection to 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_iris = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalComponents_iris = pca_iris.fit_transform(x_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf_iris = pd.DataFrame(data=principalComponents_iris,\n",
    "                                columns=['principal component 1', 'principal component 2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf_iris = pd.concat([principalDf_iris, df_iris[['target']]], axis=1)\n",
    "finalDf_iris.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Visualize 2D Projection\n",
    "\n",
    "Use a PCA projection to 2d to visualize the entire data set. You should plot different classes using different colors or shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel('Principal Component 1', fontsize=15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize=15)\n",
    "ax.set_title('2 Component PCA', fontsize=20)\n",
    "\n",
    "iris_targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "colors = ['r', 'g', 'b']\n",
    "for target, color in zip(iris_targets, colors):\n",
    "    indicesToKeep = finalDf_iris['target'] == target\n",
    "    ax.scatter(finalDf_iris.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf_iris.loc[indicesToKeep, 'principal component 2']\n",
    "               , c=color\n",
    "               , s=50)\n",
    "ax.legend(iris_targets)\n",
    "ax.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iris-setosa is linearry separablo from others class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance\n",
    "\n",
    "The explained variance tells us how much information (variance) can be attributed to each of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_iris.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together, the first two principal components contain 95.80% of the information. The first principal component contains 72.77% of the variance and the second principal component contains 23.03% of the variance. The third and fourth principal component contained the rest of the variance of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### limitations of PCA\n",
    "\n",
    "* PCA is not scale invariant. check: we need to scale our data first.\n",
    "    \n",
    "* The directions with largest variance are assumed to be of the most interest\n",
    "\n",
    "* Only considers orthogonal transformations (rotations) of the original variables\n",
    " \n",
    "* PCA is only based on the mean vector and covariance matrix. Some distributions (multivariate normal) are characterized by this, but some are not.\n",
    "\n",
    "* If the variables are correlated, PCA can achieve dimension reduction. If not, PCA just orders them according to their variances.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises - Perform PCA for breast cancer dataset\n",
    "\n",
    "* You can find this dataset it in the scikit learn library, import it and convert to pandas dataframe, original label are '0' and '1' for better readability change these names to: 'benign' and 'malignant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Załadowanie zbioru danych breast_cancer oraz zmapowanie etykiety target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sklearn_dataset_to_pandas(sklearn_dataset, target_map = None):\n",
    "    cancer_features = sklearn_dataset['data']\n",
    "    cancer_features_norm = StandardScaler().fit_transform(cancer_features)\n",
    "\n",
    "    pandas_df = pd.DataFrame(data=cancer_features_norm, columns=sklearn_dataset['feature_names'])\n",
    "    pandas_df['target'] = pd.Series(sklearn_dataset['target'])\n",
    "\n",
    "    if target_map is not None:\n",
    "        pandas_df['target'] = pandas_df['target'].map(lambda t: target_map[t])\n",
    "\n",
    "    return pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset = load_breast_cancer()\n",
    "cancer_target_map = {0: 'benign', 1: 'malignant'}\n",
    "cancer_df = sklearn_dataset_to_pandas(cancer_dataset, cancer_target_map)\n",
    "cancer_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Visualizes correlations between pairs of features (due to the greater number of features use pandas corr () function instead of pairplot instead of seaborn heatmap ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perform PCA and visualize the data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Zdefiniowanie metody do redukcji wymiaru danych przy pomocy PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_pca(n_components, df):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_components = pca.fit_transform(df.loc[:, df.columns != 'target'])\n",
    "\n",
    "    columns_names = ['Component ' + str(i) for i in range(1, n_components+1)]\n",
    "    pca_df = pd.DataFrame(data=pca_components, columns=columns_names)\n",
    "    pca_df = pd.concat([pca_df, df['target']], axis=1)\n",
    "\n",
    "    return pca_df, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pca(df):\n",
    "    targets = df['target'].unique()\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_xlabel('Component 1', fontsize=15)\n",
    "    ax.set_ylabel('Component 2', fontsize=15)\n",
    "    ax.set_title('2 Component PCA', fontsize=20)\n",
    "\n",
    "    for target, color in zip(targets, colors):\n",
    "        indicator = df['target'] == target\n",
    "        ax.scatter(\n",
    "            df.loc[indicator, 'Component 1'],\n",
    "            df.loc[indicator, 'Component 2'],\n",
    "            cmap='PuOr', s=50)\n",
    "\n",
    "    ax.legend(targets)\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cancer_df, pca_cancer = create_pca(2, cancer_df)\n",
    "visualize_pca(pca_cancer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Examine  explained variance, draw a plot showing relation between total explained variance and number of principal components used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compare_pc(df):\n",
    "    x, y = [], []\n",
    "    for i in range(2, 8):\n",
    "        pca_df, pca = create_pca(i, df)\n",
    "        total_variance = np.sum(pca.explained_variance_ratio_)\n",
    "        x.append(i)\n",
    "        y.append(total_variance)\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compare_pc(cancer_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Wraz ze zwiększaniem parametru n_components wzrastała nam całkowita wariancja. Wynika to z faktu, że zwiększa się wielkośc wektora wariancji, a każda z jego składowych zawiera niezerowe wartości. W związku z tym wraz ze zwiększaniem n_components, do sumy wariancji dochodzą nam nowe składowe. Wartość sumy wariancji dąży do 1.00, każda kolejna składowa w wektore wariancji ma coraz mniejsze wartości."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use recursive feature elimination (available in scikit-learn module) or another feature ranking algorithm to split 30 features to on 15 \"more important\" and \"less important\" features. Then repeat the last step from the full data set - draw a plot showing relation between total explained variance and number of principal components used for all 3 cases. Explain the result briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Z naszych cech wybieramy tylko 15 'najelpszych' przy użyciu algorytmu RFE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=15)\n",
    "rfe.fit(cancer_df[cancer_dataset['feature_names']].values, cancer_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorted_predictors_names = []\n",
    "for x, y in (sorted(zip(rfe.ranking_ , cancer_dataset['feature_names']), key=itemgetter(0))):\n",
    "    print(x, y)\n",
    "    sorted_predictors_names.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cancer_predictors = rfe.transform(cancer_df[cancer_dataset['feature_names']].values)\n",
    "\n",
    "cancer_df_reduce = pd.DataFrame(data=cancer_predictors, columns=sorted_predictors_names[:15])\n",
    "cancer_df_reduce['target'] = cancer_df['target']\n",
    "cancer_df_reduce.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pca_cancer_reduce_df, pca_cancer_reduce = create_pca(2, cancer_df_reduce)\n",
    "visualize_pca(pca_cancer_reduce_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Podobnie jak w przypadku PCA z wykorzystaniem wszystkich paramnetrów, widać że zbiory 'benign' oraz 'malignant' są od siebie odseparowane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compare_pc(cancer_df_reduce)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Algorytm PCA uzyskła lepszy efekt w przypadku, gdy skorzystaliśmy z 15 najlepsyzch cech wybranych przez algorytm RFE. W poprzednim przykładzie osiągnieliśmy sumy wariancji na poziomie : (0.6, 0.9), a dla modelu z 15 najlepszymi parametrami jest to : (0.7, 0. 95). Wynika to z faktu, że nie braliśmy pod uwagę cech, ktore są słabowe skorelowane z target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA\n",
    "\n",
    "PCA is a linear method. That is it can only be applied to datasets which are linearly separable. It does an excellent job for datasets, which are linearly separable. But, if we use it to non-linear datasets, we might get a result which may not be the optimal dimensionality reduction. Kernel PCA uses a kernel function to project dataset into a higher dimensional feature space, where it is linearly separable. It is similar to the idea of Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.02, random_state=417)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s apply PCA on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.title(\"PCA\")\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA failed to distinguish the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "kpca = KernelPCA(kernel='rbf', gamma=15)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "plt.title(\"Kernel PCA\")\n",
    "plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying kernel PCA on this dataset with RBF kernel with a gamma value of 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KernelPCA exercises\n",
    "\n",
    "* Visualize in 2d datasets used in this labs, experiment with the parameters of the KernelPCA method change kernel and gamma params. Docs: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Zaczniemy od KernelPCA dla Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def apply_kernel_pca(df, gamma=15):\n",
    "    features = df.loc[:, df.columns != 'target']\n",
    "    k_pca = KernelPCA(kernel='rbf', gamma=gamma)\n",
    "    features_k_pca = k_pca.fit_transform(features)\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    label_encoder.fit(df.target)\n",
    "    label_colors= label_encoder.transform(df.target)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_title('Kernel PCA', fontsize=20)\n",
    "\n",
    "    ax.scatter(features_k_pca[:, 0], features_k_pca[:, 1], c=label_colors)\n",
    "    ax.legend(df.target)\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "apply_kernel_pca(df_iris_standarize, gamma=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Podobnie jak w przypadku PCA widzimy, że Iris-setosa jest odseparowany od dwóch pozostałych klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "apply_kernel_pca(cancer_df_reduce, gamma=0.5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "#Jakiś komentatz do tego wykresy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "* Download the MNIST data set (there is a function to load this set in libraries such as scikit-learn, keras). It is a collection of black and white photos of handwritten digits with a resolution of 28x28 pixels. which together gives 784 dimensions.\n",
    "\n",
    "* Try to visualize this dataset using PCA and KernelPCA, don't expect full separation of the data\n",
    "\n",
    "* Similar to the exercises, examine explained variance. draw explained variance vs number of principal Components plot.\n",
    "\n",
    "* Find number of principal components for 99%, 95%, 90%, and 85% of explained variance.\n",
    "\n",
    "* Draw some sample MNIST digits and from PCA of its images transform data back to its original space (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.inverse_transform). Make an inverse transformation for number of components coresponding with explained variance shown above and draw the reconstructed images. The idea of this exercise is to see visually how depending on the number of components some information is lost.\n",
    "\n",
    "* Perform the same reconstruction using KernelPCA (make comparisons for the same components number)\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA.inverse_transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links\n",
    "https://scikit-learn.org\n",
    "https://towardsdatascience.com/introduction-to-principal-component-analysis-pca-with-python-code-69d3fcf19b57\n",
    "https://towardsdatascience.com/kernel-pca-vs-pca-vs-ica-in-tensorflow-sklearn-60e17eb15a64"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Przygotowanie zbioru danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mnist_dataset = load_digits()\n",
    "mnist_df = sklearn_dataset_to_pandas(mnist_dataset)\n",
    "mnist_df.head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Użycie algorytmu PCA na zbiorze MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pca_mnist_df, pca_mnist = create_pca(2, mnist_df)\n",
    "visualize_pca(pca_mnist_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Ogólnie rzecz biorąc to ciężko dopatrzeć się pełnej separacji danych klas. Napewno można dostrzec, ża najbardziej odseparowane od pozostałych sa klasa 4 oraz 6. Najbardziej skorelowane z pozostalymi klasami są 8 oraz 9."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Użycie algorytmu Kernel PCA na zbiorze MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "apply_kernel_pca(mnist_df, gamma=0.01)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Kernel PCA zadziałał już lepiej, eksperymentując z paramterem gamma można uzyskać dość widoczną separacją na dwie części : część po prawej (nie wiem jakie liczby bo legenda nie działa) i cześć po lewej. Dodatkowo w części po prawej mamy całkiem ładną rozróżnienie 3 różnych klas."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Porównanie total variance w przypadku użycia różnej liczby n_components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compare_pc(mnist_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Total variance rośnie, ale widać że jest słabo. Dla n=8 użyskaliśmy tylko total_variance na poziomie 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Find number of principal components for 99%, 95%, 90%, and 85% of explained variance.\n",
    "mnist_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
